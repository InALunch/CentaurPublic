# CentaurPublic
An experimental setup to do hybrid human forecasting + modeling for covid-19 forecasts

Our endpoint is total JHU confirmed deaths in the US, as detailed here: https://covid19forecasthub.org/

We are outputting forecasts early on. We expect the early forecasts to be crap, but doing them to still provide valuable information for our own processes. 

1-3 weeks from now, we'll re-evaulate and likely decide to do one of the following:

1) We are pessimistic enough about the project/don't like doing this project enough (eg because of lack of trust in the process, issues with team dynamics, or because we have better things to do with our time), and will scrape it.(internal validity)
2) We think that not only are our initial forecasts crap, but we do not see any trajectory for them to be better than existing forecasts on the Covid-19 Forecasting Hub, so our added value is zero or negative (external validity)
3) We think on balance we have a good process that can be improved with additional learnings and manpower, and are optimistic we can add value to the existing forecasts. In such a world, we'll add on additional volunteers and go through the process of submitting to Covid 19 Forecast Hub. (continue and scale)
4) We decide that the project looks reasonable and as a team finds more important things to forecast on. (pivot)

As with most startup/volunteer projects, the base rate is that we will fail. Hopefully by publishing our initial artifacts and thoughts before we know the final outcome, we can help fight against reporting/survivorship/publication bias.

Update Week 1: Lots of growing pains, but overall we nailed a process down so we hope to improve on future weeks.

Update Week 2: The forecasts were very rushed for this week. We learned some lessons from last week, but overall less time was put on this week's forecasts, so we expect this week's forecasts to perform worse than last week's or the next week's.

We evaluated last week's forecasts. We caught one glaring error (Georgia median) and a few minor ones, but overall things look reasonable (8/12 states in our 50% CI, 11/12 states in our 90% CI, mean absolute error not too bad). 

We did our first internal evaluation. We currently plan to go forward with 8 states (rather than 12) for the last few weeks, so we can nail a process/high accuracy down, and scale only after we have internal and external confidence in our own accuracy.
